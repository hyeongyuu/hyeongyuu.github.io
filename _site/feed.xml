<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-03-14T15:31:08+09:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hyeongyu.log</title><subtitle>Self blog</subtitle><author><name>Moon Hyeongyu</name></author><entry><title type="html">FixMatch</title><link href="http://localhost:4000/machine%20learning/fixmatch/" rel="alternate" type="text/html" title="FixMatch" /><published>2023-03-11T00:00:00+09:00</published><updated>2023-03-11T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/fixmatch</id><content type="html" xml:base="http://localhost:4000/machine%20learning/fixmatch/"><![CDATA[<h2 id="시작하며">시작하며</h2>
<p>최근 지도학습(Supervised learning) 접근으로 모델 개발을 진행하고 있습니다. 1차원 시계열 데이터로 한 개의 패턴을 인식하는 모델 개발이 목표이며, 패턴 인식으로 특정 시간 내의 패턴 개수, 길이 등을 파악하고자 합니다. 입력값(input data)으로 시계열 데이터가 들어가면 출력값(output data)으로 패턴 인식 결과를 반환하는 방식으로 내부 DB의 약 1,000개를 활용하여 모델 개발을 진행하고 있습니다.</p>

<p>데이터는 몇 달 동안 서비스를 제공하면서 축적하여 데이터가 약 1,000개 정도로 어느 정도 크기가 있지만, 신규 고객사가 생겨 새로운 모델을 개발한다면 또 다른 훈련을 위한 데이터, 즉 새로운 고객사 데이터가 필요할 것입니다. 이를 해결하기 위해 데이터가 많이 없어도 그만큼의 효과를 낼 수 있는 준지도학습(Semi-supervised learning) 접근으로 모델을 개발하고자 합니다.</p>

<p>이번 글에서 준지도학습 도입을 위해 준지도학습은 어떤 훈련 방식인지? 어떨 때 사용하면 좋을지? 등에 대해 알아보고, 방법 중 하나인 FixMatch에 대해 작성하고자 합니다. FixMatch의 핵심이 되는 부분에 대해 정리하고, 직접 코드로 구현하면서 자세히 알아보도록 하겠습니다.</p>

<h2 id="준지도학습semi-supervised-learning">준지도학습(Semi-supervised learning)</h2>
<h3 id="준지도학습이란">준지도학습이란?</h3>
<p><span style="background-color: #dcffe4"><strong>준지도학습은 <u>레이블이 있는 데이터(Labeled data)</u>와 <u>레이블이 없는 데이터(Unlabeled data)</u>를 조합하여 모델을 훈련하는 기법</strong></span>입니다. <strong>지도학습</strong>은 레이블이 있는 데이터만 훈련하지만, <strong>준지도학습</strong>은 레이블이 없는 데이터를 함께 사용하여 <strong><u>모델이 더 많은 데이터를 학습</u></strong>하도록 합니다. 아래 이미지에서 보듯 레이블이 없는 데이터를 훈련에 사용하기 때문에 최적의 decision boundary를 찾을 수 있습니다.</p>

<p><img src="/assets/images/post_images/semi_supervised_learning.png" alt="Semi-supervised learning" width="600" height="600" class="align-center" /></p>

<p><strong>준지도학습의 핵심</strong>은 <strong>레이블이 있는 데이터와 없는 데이터</strong>를 <strong>모두 모델 훈련에 사용하는 것</strong>입니다. <strong>레이블이 있는 데이터</strong>로 모델의 입력값, 출력값 간의 관계를 학습하여 <strong>확률분포를 추정</strong>하고, <strong>레이블이 없는 데이터</strong>로 추정한 <strong>확률분포의 주변값을 학습</strong>함으로써 모델의 일반화 능력을 향상시킬 수 있습니다.</p>

<p>준지도학습으로 개, 고양이 분류 모델을 개발한다고 가정해보겠습니다. 제가 보유한 이미지가 총 100장이고 이 중에서 개, 고양이로 표시된(레이블이 있는) 이미지가 10장 있습니다. 먼저 10장(레이블이 있는 이미지)으로 개, 고양이를 알려주고 나머지 90장(레이블이 없는 이미지)에 대해서는 이전 10장을 기반으로 나머지 90장을 알려주는 것입니다. 그 결과, 실제로 정답지는 10개밖에 없었지만, 100개를 전부 학습한 효과를 얻게 됩니다.</p>

<h3 id="준지도학습의-필요성-및-효과">준지도학습의 필요성 및 효과</h3>
<p>웹상에서 얻을 수 있는 데이터는 많지만 <strong>목적에 맞게 레이블까지 있는 데이터는 구하기 어렵습니다.</strong> 예를 들어 안경 쓴 고양이에 대한 이미지를 얻고자 한다면, 아래와 같이 구글 검색만으로 여러 장의 이미지 데이터를 수집할 수 있습니다. 그러나 이미지 한 장씩 안경 쓴 고양이인지 아닌지 직접 확인하고 태깅하며 데이터셋을 구축하는 작업은 비용이 많이 소모됩니다.</p>

<p><img src="/assets/images/post_images/google_cat.png" alt="Google cat" width="450" height="600" /> -&gt; <strong>Tagging</strong> <img src="/assets/images/post_images/labeled_cat.png" alt="Labeled cat" width="300" height="300" /></p>

<p>이와같이 <strong>데이터를 얻는 데 시간과 비용이 많이 요구되는 경우 준지도학습이 적합</strong>합니다. 앞선 예시처럼 10개의 레이블이 있는 이미지로 100개만큼의 학습 효과를 얻는 것처럼 적은 양의 데이터로 많은 양의 데이터를 학습한 효과를 가져올 수 있기 때문입니다.</p>

<p>그리고 <strong>상대적으로 구하기 쉬운 레이블이 없는 데이터를 어떻게 활용할 것인가에 대한 해결책이 준지도학습</strong>이라 생각합니다. 레이블이 없는 데이터는 레이블이 있는 데이터에 비해 많은 양의 데이터 확보가 가능하므로 이를 전부 학습에 사용한다면 더 뛰어난 성능을 가진 모델을 생성할 수 있을 것입니다.
<!-- 기초 확률분포가 정확할수록 좋은 효과를 가져옴, 레이블 데이터의 부족, 확률분포 추정 --></p>

<h2 id="fixmatch">Fixmatch</h2>
<h3 id="fixmatch란">Fixmatch란?</h3>
<p><span style="background-color: #dcffe4"><strong>준지도학습 방법론 중 하나로 일관성(Consistency)과 신뢰성(Confidence)에 기반한 FixMatch</strong></span>가 있습니다. <strong>데이터를 변형해도 고유한 성질은 유지하고 있다는 일관성</strong>(ex. 강아지 이미지를 아무리 변형해도 강아지라는 사실)과 <strong>변형한 데이터의 예측값에 대한 신뢰성</strong>(ex. 강아지라는 예측값을 얼마나 믿을 수 있는지)이 핵심적인 부분이며, <strong>데이터를 서로 다른 방식으로 변형 및 대조하는 방식으로 모델을 학습</strong>합니다.</p>

<p><img src="/assets/images/post_images/fixmatch_pipeline.png" alt="FixMatch pipeline" width="600" height="600" class="align-center" /></p>

<p><strong>모델 학습</strong>은 아래와 같이 진행합니다.</p>
<ol>
  <li>레이블이 있는 데이터로 모델 학습</li>
  <li>학습된 모델로 레이블링 되지 않은 데이터 예측값 생성
    <ul>
      <li><strong>Weak Augmentation</strong>. 약한 변형으로 <u>confidence threshold를 넘는 결과에 대해 Pseudo-label 생성</u></li>
      <li><strong>Strong Augmentation</strong>. 강한 변형으로 <u>앞서 생성한 Pseudo-label로 loss 산출</u></li>
    </ul>
  </li>
  <li>레이블이 있는 데이터와 없는 데이터의 loss를 합하여 모델 학습
    <ul>
      <li><strong>Loss(total) = Loss(Labeled data) + Loss(Unlabeled data)</strong></li>
    </ul>
  </li>
</ol>

<h4 id="consistency-regularization">Consistency regularization</h4>
<p>대부분의 준지도학습에서 사용하는 방법으로 <strong><u>데이터에 작은 변형을 가해도 모델 예측값은 동일</u></strong>하다는 것입니다. 고양이 이미지에 <strong>노이즈를 추가해도 데이터의 고유한 성질은 해치지 않기 때문에</strong> 모델은 이전과 동일하게 고양이로 예측할 것입니다.</p>

<p><img src="/assets/images/post_images/consistency_regularization.png" alt="Consistency regularization" width="600" height="600" class="align-center" /></p>

<p>Consistency regularization은 FixMatch의 가장 중요한 방법론으로 <strong>레이블이 있는 데이터의 주변 확률 분포에 대해 학습하는 효과</strong>를 가져오는 동시에 <strong>Pseudo-label을 생성하기 위한 가정</strong> 중 하나이기도 합니다.</p>

<h4 id="pseudo-labeling">Pseudo labeling</h4>
<p>모델로 레이블이 없는 데이터를 예측하고, <strong>예측값을 레이블로 생성하여 기존의 학습 데이터와 생성한 레이블을 함께 모델 학습에 사용하는 방법</strong>입니다. 모델로 생성한 레이블이 정확하지 않을 수 있으므로 데이터의 크기와 모델 성능을 고려하여 아래 두 가지 방법 중 적절한 방법을 선정하는 것이 중요합니다.</p>

<p>준지도학습의 필수 방법론으로 FixMatch에서 Consistency regularization과 결합하여 사용함으로써 더 큰 학습 효과를 가져옵니다.</p>

<p><strong>Method 1. 레이블을 생성해나가며 학습</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 레이블이 있는 데이터와 없는 데이터를 함께 사용하며 학습을 진행
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="mi">0</span>    <span class="c1"># 손실함수에 대한 가중치 loss = labeled loss + (alpha * unlabeled loss)
</span>
<span class="k">for</span> <span class="n">train_data</span><span class="p">,</span> <span class="n">unlabel_data</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">trainloader</span><span class="p">,</span> <span class="n">unlabeledloader</span><span class="p">):</span> 
    
    <span class="n">inputs</span><span class="p">,</span> <span class="n">_inputs</span><span class="p">,</span> <span class="n">labels</span> <span class="o">=</span> <span class="n">train_data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">unlabel_data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">),</span> <span class="n">train_data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">zero_grad</span><span class="p">()</span>

    <span class="n">outputs</span><span class="p">,</span> <span class="n">_outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">inputs</span><span class="p">),</span> <span class="n">model</span><span class="p">(</span><span class="n">_inputs</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">_outputs</span><span class="p">.</span><span class="n">detach</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span>  <span class="o">+</span> <span class="n">alpha</span><span class="o">*</span><span class="n">criterion</span><span class="p">(</span><span class="n">_outputs</span><span class="p">,</span> <span class="n">_labels</span><span class="p">)</span>
    <span class="n">loss</span><span class="p">.</span><span class="n">backward</span><span class="p">()</span>
    <span class="n">optimizer</span><span class="p">.</span><span class="n">step</span><span class="p">()</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">detach</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>

    <span class="n">total</span> <span class="o">+=</span> <span class="n">labels</span><span class="p">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">correct</span> <span class="o">+=</span> <span class="p">(</span><span class="n">predicted</span> <span class="o">==</span> <span class="n">labels</span><span class="p">).</span><span class="nb">sum</span><span class="p">().</span><span class="n">item</span><span class="p">()</span>
</code></pre></div></div>
<p><strong>Method 2. 학습된 모델로 레이블 생성</strong></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 레이블이 있는 데이터로 모델을 생성한 다음 레이블이 없는 데이터로 학습을 진행
</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">().</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
<span class="n">model</span><span class="p">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="s">'pseudo_label_model.pth'</span><span class="p">))</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="p">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">model</span><span class="p">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">)</span>

<span class="n">pseudo_threshold</span> <span class="o">=</span> <span class="mf">0.95</span> <span class="c1"># 레이블 유의수준 confidence threshold
</span><span class="n">pseudo_images</span><span class="p">,</span> <span class="n">pseudo_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">float</span><span class="p">),</span> <span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">([],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="p">.</span><span class="nb">long</span><span class="p">)</span>
 
<span class="k">with</span> <span class="n">torch</span><span class="p">.</span><span class="n">no_grad</span><span class="p">():</span>
    <span class="k">for</span> <span class="n">data</span> <span class="ow">in</span> <span class="n">unlabeledloader</span><span class="p">:</span>
        <span class="n">model</span><span class="p">.</span><span class="nb">eval</span><span class="p">()</span>
        <span class="n">images</span> <span class="o">=</span> <span class="n">data</span><span class="p">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">model</span><span class="p">(</span><span class="n">images</span><span class="p">)</span>
        <span class="n">outputs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">functional</span><span class="p">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="n">max_val</span><span class="p">,</span> <span class="n">predicted</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">detach</span><span class="p">(),</span> <span class="mi">1</span><span class="p">)</span>
        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">max_val</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span> <span class="o">&gt;=</span> <span class="n">pseudo_threshold</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">pseudo_images</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pseudo_images</span><span class="p">,</span> <span class="n">images</span><span class="p">.</span><span class="n">cpu</span><span class="p">()[</span><span class="n">idx</span><span class="p">]),</span> <span class="mi">0</span><span class="p">)</span> 
            <span class="n">pseudo_labels</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">cat</span><span class="p">((</span><span class="n">pseudo_labels</span><span class="p">,</span> <span class="n">predicted</span><span class="p">.</span><span class="n">cpu</span><span class="p">()[</span><span class="n">idx</span><span class="p">]),</span> <span class="mi">0</span><span class="p">)</span>
</code></pre></div></div>

<h4 id="loss-function">Loss function</h4>
<p><strong>FixMatch 손실함수(Loss function)은 <u>레이블이 있는 데이터의 loss</u>와 <u>레이블이 없는 데이터의 loss</u>를 함께 사용</strong>합니다. 레이블이 있는 데이터는 지도학습과 같은 방법으로 loss를 산출하지만, <strong>레이블이 없는 데이터는 weak augmentation으로 생성한 레이블에 strong augmentation의 예측값으로 loss를 산출</strong>합니다. 그리고 일정 확률 이상의 정확한 레이블만 사용하거나 레이블이 없는 데이터의 loss에는 가중치를 부여하는 등의 제약 조건을 설정하여 loss function의 정확도를 높입니다.</p>

<center>$L_{total}=L_{labeled}+\lambda L_{unlabeled}$ ($\lambda$는 unlabeled data의 loss에 대한 가중치입니다.)<br /><br />$L_{labeled}=-\frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{C} y_{i,j} \log(p_{i,j})$, $L_{unlabeled}=-\frac{1}{N}\sum_{i=1}^{N}y_{i} \log(p_{i})$</center>
<p><br /></p>

<p>$L_{labeled}$(레이블이 있는 데이터의 loss)는 cross-entropy loss를 사용하여 계산합니다. $N$은 배치 크기, $C$는 클래스 수, $y_{i,j}$는 $i$번째 샘플의 $j$번째 클래스 레이블(one-hot encoding), $p_{i,j}$는 $i$번째 샘플의 $j$번째 클래스에 속할 확률을 나타냅니다. 즉, $p_{i,j}=\text{softmax}(z_{i,j})$이며, $z_{i,j}$는 모델의 출력값입니다.</p>

<p>$L_{unlabeled}$(레이블이 없는 데이터의 loss)는 pseudo-label을 사용하여 계산합니다. $N$은 배치 크기, $y_{i}$는 $i$번째 샘플의 pseudo-label을 나타내며, $p_{i}$는 $i$번째 샘플의 pseudo-label에 속할 확률을 나타냅니다. 즉, $p_{i}=\text{softmax}(z_{i})$이며, $z_{i}$는 모델의 출력값입니다.</p>

<!-- ### 코드 구현
Pytorch로 구현한 FixMatch 코드입니다. -->

<h2 id="마치며">마치며</h2>
<p>이번 글에서 준지도학습과 그 중 하나인 FixMatch에 대해 알아보았습니다. 준지도학습은 어떤 학습 방법인지, 그 중 FixMatch는 어떤 로직으로 모델을 학습하는지와 같이 이론적인 부분을 글로 정리하면서 깊게 이해할 수 있었습니다. 반대로 로직을 코드로 구현하고 모델 개발까지 진행하여 실제 성능을 비교 및 검증해보지 못한 부분이 아쉬웠습니다.</p>

<p>다음 글에서는 준지도학습에 필요한 최소한의 데이터는 어느정도인지, 레이블이 있는 데이터와 없는 데이터의 비율은 얼만큼이 적합할지 등 실험을 통해 모델의 한계점과 문제점을 파악해보면 좋을 것 같습니다.</p>

<!-- ## Reference -->]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Semi-supervised learning" /><summary type="html"><![CDATA[시작하며 최근 지도학습(Supervised learning) 접근으로 모델 개발을 진행하고 있습니다. 1차원 시계열 데이터로 한 개의 패턴을 인식하는 모델 개발이 목표이며, 패턴 인식으로 특정 시간 내의 패턴 개수, 길이 등을 파악하고자 합니다. 입력값(input data)으로 시계열 데이터가 들어가면 출력값(output data)으로 패턴 인식 결과를 반환하는 방식으로 내부 DB의 약 1,000개를 활용하여 모델 개발을 진행하고 있습니다.]]></summary></entry><entry><title type="html">Augmentation of Time series data</title><link href="http://localhost:4000/machine%20learning/augmentation_timeseries/" rel="alternate" type="text/html" title="Augmentation of Time series data" /><published>2023-02-25T00:00:00+09:00</published><updated>2023-02-25T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/augmentation_timeseries</id><content type="html" xml:base="http://localhost:4000/machine%20learning/augmentation_timeseries/"><![CDATA[<h2 id="시작하며">시작하며</h2>
<!-- 도입부 -->
<p><strong>시계열 데이터</strong>는 <strong><u>일정한 시간 간격으로 발생하는 시간에 따라 정렬된 데이터</u></strong>를 의미하며, 오늘날 제조, 금융, 의료 등 다양한 분야에서 발생하고 있습니다. 그리고 현업에서 머신러닝(회귀, 분류)을 위한 시계열 데이터는 다른 분야(이미지, 텍스트 등)와 같은 목적으로 활용됩니다.</p>

<p>다른 분야는 웹상에 공유된 데이터도 많고 바로 서비스 적용할 수 있는 수준의 모델도 많아 개발에 어려움이 없지만, 시계열 분야는 <strong>데이터도 별로 없을뿐더러</strong> 분류, 예측 등 목적에 특화된 <strong>모델 개발도 많이 진행되지 못해</strong> 개발에 어려움이 많습니다.</p>

<p>이러한 문제 해결 방법의 하나인 <strong>데이터 부족 문제</strong>를 해결하는 <span style="background-color: #dcffe4"><strong>데이터 증강(Data augmentation)</strong></span>에 대해 알아보고자 합니다.</p>

<h2 id="데이터-증강data-augmentation">데이터 증강(Data augmentation)</h2>
<h3 id="데이터-증강이란">데이터 증강이란?</h3>
<p><strong>데이터 증강</strong>이란 <strong><u>데이터양을 늘리기 위한 기법</u></strong>으로, 보유한 데이터에 여러 변환을 적용하여 데이터 수를 증가시킵니다. Computer Vision 분야에서 필수로 사용되며, 아래 이미지와 같이 원본 이미지를 다양한 방식으로 변환해 원본 데이터에서 학습하지 못했던 부분을 모델이 학습할 수 있도록 합니다.<br /></p>

<p><img src="/assets/images/post_images//woong.png" alt="Woong" /> ➡️ <img src="/assets/images/post_images//woong_augmentation.png" alt="Woong_augmentation" /><br /></p>

<p><strong>데이터 증강</strong>으로 원본 이미지의 고유한 특성(고양이)은 유지한 새로운 이미지(고양이)를 생성하였고, 그 결과 <span style="background-color: #dcffe4">한정적인 훈련 <strong>데이터의 양을 증가</strong>시켜 모델 성능을 높이고 과적합 문제를 해결</span>할 수 있습니다.</p>

<h3 id="시계열-데이터-증강">시계열 데이터 증강</h3>
<p>데이터 증강은 시계열 데이터에도 적용할 수 있습니다. 시계열 데이터의 경우 데이터 수집이 어렵고 비용이 많이 들기 때문에 데이터 증강 기술이 유용하게 활용됩니다.</p>

<p>일반적으로 시간이 지남에 따라 변화하는 시간 축을 가지고 있으므로, <u>증강 기법은 시간 축을 활용하여 데이터를 변형</u>합니다. 시간 축을 <strong>이동하거나(Shift)</strong>, <strong>회전시키거나(Rotation)</strong>, <strong>노이즈를 추가하거나(Noise)</strong>, <strong>일부 구간을 제거하거나(Crop)</strong>, <strong>데이터를 뒤집는(Reverse)</strong> 등의 다양한 기법을 적용할 수 있습니다.</p>

<p>이를 통해 새로운 시계열 데이터를 생성하고 모델 학습에 이용해 데이터의 다양성을 확보하고 성능을 향상시킬 수 있습니다.</p>

<h2 id="데이터-증강-기법">데이터 증강 기법</h2>
<p><strong>시계열 데이터 증강 기법</strong>은 크게 원본 데이터를 <strong><u>단순 변형하는 방법</u></strong>과 <strong><u>생성 모델(Generative model)</u></strong> 개발을 통한 새로운 데이터를 생성하는 방법이 있습니다. 본 포스팅에선 기존 데이터를 <strong>단순 변형하는 방법</strong>에 대해 알아보고, 증강 기법을 코드로 구현 및 시각화 자료로 결과를 확인하고자 합니다.
<img src="/assets/images/post_images//timeseries_augmentation.png" alt="Timeseries augmentation" /></p>

<h3 id="1-cropping">1. Cropping</h3>
<p><strong>Cropping</strong>은 <strong><u>원본 데이터의 일부 구간을 잘라내는 기법</u></strong>입니다. 데이터 일부분을 제거하여 정보 손실이 있지만, 제거한 부분이 원본 데이터에서 불필요한 부분이라면 학습에 최적화된 데이터를 생성 및 활용하는 것으로 볼 수 있습니다. 이처럼 cropping을 통해 원본 데이터의 특징을 도출하고 불필요한 데이터를 제거한 새로운 데이터를 생성합니다.</p>

<h4 id="time-cropping">Time cropping</h4>
<p>전체 구간을 cropping 하는 방법으로 데이터의 시작, 끝부분을 특정 길이만큼 제거합니다. 전체 길이의 cropping 할 percentage를 입력하여 시작, 끝부분에 각각 적용합니다. 그 결과 아래와 같이 불필요한 데이터를 제거할 수 있습니다. 
<img src="/assets/images/post_images//time_cropping.png" alt="Time cropping" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">time_crop</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">pct</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="s">"""
    inputs
        series
            a pandas Series, must have DatetimeIndex
        pct
            a cropping percentage, within interval [0.0, 0.5] (default 0.1)
    output
        series
            a pandas Series with DatetimeIndex
    """</span>
    <span class="n">sr_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">)</span>
    <span class="n">start</span><span class="p">,</span> <span class="n">end</span> <span class="o">=</span> <span class="p">(</span><span class="nb">int</span><span class="p">(</span><span class="n">sr_len</span><span class="o">*</span><span class="n">p</span><span class="p">)</span> <span class="k">for</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pct</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">series</span><span class="p">[</span><span class="n">start</span><span class="p">:</span><span class="o">-</span><span class="n">end</span><span class="p">]</span>
</code></pre></div></div>

<h4 id="window-cropping">Window cropping</h4>
<p>데이터를 일정한 간격으로 cropping 하는 방법으로 데이터 일부분을 제거합니다. 시작, 끝부분만을 제거하는 Time cropping과 다르게 전체적으로 균등하게 데이터를 제거합니다.
<img src="/assets/images/post_images//window_cropping.png" alt="Window cropping" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">window_crop</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">freq</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">pct</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="s">"""
    inputs
        series
            a pandas Series, must have DatetimeIndex
        freq
            a frequency integer to be used as cropping difference (default 100)
        pct
            a cropping percentage, within interval [0.0, 0.5] (default 0.1)
    output
        series
            a pandas Series with DatetimeIndex
    """</span>
    <span class="n">sr_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">sr_len</span><span class="p">)</span>
    <span class="n">crop_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">freq</span> <span class="o">*</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">uniform</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">pct</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">in_freq</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">((</span><span class="n">freq</span> <span class="o">*</span> <span class="n">i</span><span class="p">),</span> <span class="p">(</span><span class="n">freq</span> <span class="o">*</span> <span class="p">(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">))</span> <span class="o">-</span> <span class="n">crop_size</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">j</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">idx</span><span class="p">[</span><span class="n">freq</span><span class="p">::</span><span class="n">freq</span><span class="p">])]).</span><span class="n">ravel</span><span class="p">()</span>
    <span class="n">out_freq</span> <span class="o">=</span> <span class="n">idx</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">sr_len</span> <span class="o">%</span> <span class="n">freq</span><span class="p">):]</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">in_freq</span><span class="p">,</span> <span class="n">out_freq</span><span class="p">))</span>
    <span class="k">return</span> <span class="n">series</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span>
</code></pre></div></div>
<h3 id="2-warping">2. Warping</h3>
<p><strong>Warping</strong>은 <strong><u>원본 데이터의 일부 구간을 압축 또는 확장시키는 기법</u></strong>입니다. 시계열 데이터의 시간 축을 일정한 시간만큼 이동시켜 새로운 데이터를 생성하는 방법으로 모델이 데이터의 시간 축 변동에 대해 잘 학습할 수 있도록 도와줍니다. 예를 들어, 특정 이벤트가 발생한 시점에서 데이터의 패턴이 변화할 수 있습니다. 이러한 패턴을 모델이 잘 학습하기 위해 시간 축을 이동시켜 새로운 변화된 패턴을 생성합니다.</p>
<h4 id="time-warping">Time warping</h4>
<p>전체 구간을 warping 하는 방법으로 전체 데이터를 서로 다른 n개의 데이터로 분할하고, 분할된 데이터를 압축 또는 확장시켜 새로운 데이터를 생성합니다.
<img src="/assets/images/post_images//time_warping.png" alt="Time warping" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">scipy.interpolate</span> <span class="kn">import</span> <span class="n">CubicSpline</span>

<span class="k">def</span> <span class="nf">time_warping</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">n_warp</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">10</span><span class="p">):</span>
    <span class="s">"""
    inputs
        series
            a pandas Series, must have DatetimeIndex
        n_warp
            a number of warping list (default 5)
        pad
            a padding length (default 10)
    output
        series
            a pandas Series with DatetimeIndex
    """</span>
    <span class="k">def</span> <span class="nf">__fit_spline</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">size</span><span class="p">):</span>
        <span class="n">spl</span> <span class="o">=</span> <span class="n">CubicSpline</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">1.00</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">y</span><span class="p">)),</span> <span class="n">y</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">spl</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.00</span><span class="p">,</span> <span class="mf">1.00</span><span class="p">,</span> <span class="n">size</span><span class="p">))</span>
    <span class="n">sr_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">pad</span><span class="p">,</span> <span class="n">sr_len</span><span class="o">-</span><span class="n">pad</span><span class="p">)</span>
    <span class="n">_ts_points</span><span class="p">,</span> <span class="n">_val_points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_warp</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_warp</span><span class="p">)</span>
    <span class="k">while</span> <span class="nb">len</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">where</span><span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">_ts_points</span> <span class="o">-</span> <span class="n">_val_points</span><span class="p">)</span><span class="o">&lt;</span><span class="n">pad</span><span class="p">)[</span><span class="mi">0</span><span class="p">]):</span>
        <span class="n">_ts_points</span><span class="p">,</span> <span class="n">_val_points</span> <span class="o">=</span> <span class="p">(</span><span class="n">np</span><span class="p">.</span><span class="n">sort</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">n_warp</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">2</span><span class="p">))</span>
    <span class="n">ts_points</span><span class="p">,</span> <span class="n">val_points</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]),</span> <span class="n">_ts_points</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">sr_len</span><span class="o">-</span><span class="mi">1</span><span class="p">]))),</span> <span class="n">np</span><span class="p">.</span><span class="n">hstack</span><span class="p">((</span><span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="mi">0</span><span class="p">]),</span> <span class="n">_val_points</span><span class="p">,</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">sr_len</span><span class="o">-</span><span class="mi">1</span><span class="p">])))</span>
    <span class="n">ts</span><span class="p">,</span> <span class="n">val</span> <span class="o">=</span> <span class="n">series</span><span class="p">.</span><span class="n">index</span><span class="p">.</span><span class="n">values</span><span class="p">,</span> <span class="n">series</span><span class="p">.</span><span class="n">values</span>
    <span class="n">lst</span> <span class="o">=</span> <span class="p">[[</span><span class="n">ts</span><span class="p">[</span><span class="n">ts_points</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">ts_points</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]],</span> <span class="n">val</span><span class="p">[</span><span class="n">val_points</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span><span class="n">val_points</span><span class="p">[</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">]]]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span><span class="n">p</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">ts_points</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">])]</span>
    <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">pd</span><span class="p">.</span><span class="n">Series</span><span class="p">(</span><span class="n">__fit_spline</span><span class="p">(</span><span class="n">_ls</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">_ls</span><span class="p">[</span><span class="mi">0</span><span class="p">])),</span> <span class="n">index</span><span class="o">=</span><span class="n">_ls</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span> <span class="k">for</span> <span class="n">_ls</span> <span class="ow">in</span> <span class="n">lst</span><span class="p">])</span>
</code></pre></div></div>
<h4 id="window-warping">Window warping</h4>
<p>데이터를 일정한 간격(window)으로 waping 하는 방법입니다. Time warping과는 다르게 전체 데이터를 균등하게 분할 및 warping 하여 상대적으로 변환 효과를 줄여 기존 특성을 최대한 보존하면서 warping을 진행합니다.
<img src="/assets/images/post_images//window_warping.png" alt="Window warping" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">window_warping</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">window</span><span class="o">=</span><span class="mf">0.2</span><span class="p">):</span>
    <span class="s">"""
    inputs
        series
            a pandas Series, must have DatetimeIndex
        window
            a percentage for window size, within interval [0.0, 0.5] (default 0.2)
    output
        series
            a pandas Series with DatetimeIndex
    """</span>
    <span class="n">sr_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">series</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sr_len</span><span class="p">)</span>
    <span class="n">freq</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">sr_len</span> <span class="o">*</span> <span class="n">window</span><span class="p">)</span>
    <span class="n">pad</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">array</span><span class="p">([</span><span class="n">np</span><span class="p">.</span><span class="n">nan</span><span class="p">]</span> <span class="o">*</span> <span class="p">((</span><span class="n">freq</span> <span class="o">-</span> <span class="p">(</span><span class="n">sr_len</span> <span class="o">%</span> <span class="n">freq</span><span class="p">))</span><span class="o">//</span><span class="mi">2</span><span class="p">))</span> 
    <span class="n">idx_lst</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">pad</span><span class="p">,</span> <span class="n">idx</span><span class="p">,</span> <span class="n">pad</span><span class="p">)).</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">freq</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">pd</span><span class="p">.</span><span class="n">concat</span><span class="p">([</span><span class="n">time_warping</span><span class="p">(</span><span class="n">series</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">_ls</span><span class="p">[</span><span class="o">~</span><span class="n">np</span><span class="p">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">_ls</span><span class="p">)]],</span> <span class="n">n_warp</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">pad</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span> <span class="k">for</span> <span class="n">_ls</span> <span class="ow">in</span> <span class="n">idx_lst</span><span class="p">])</span>
</code></pre></div></div>
<!-- #### Magnitude warping -->
<!-- 추가 작성 -->

<h3 id="3-noise-injection">3. Noise injection</h3>
<p><strong>Noise injection</strong>은 <strong><u>랜덤한 노이즈를 시계열 데이터에 추가하여 데이터를 변형시키는 기법</u></strong>입니다. 노이즈 주입은 모델이 더 다양한 패턴을 학습하도록 하며, 모델을 더욱 강건하게 만드는 효과가 있지만, 너무 많은 노이즈가 추가될 경우 모델의 성능을 저하시킬 수도 있습니다. 따라서 적절한 노이즈 크기와 종류를 선택하는 것이 중요합니다.</p>
<h4 id="white-noise-injection">White noise injection</h4>
<!-- 간단 설명, 코드, 시각화 -->
<p>White noise는 백색잡음으로 불리는 대부분 데이터에서 쉽게 발견되는 노이즈입니다. white noise injection은 원본 데이터 특성은 유지하고 자연적인 노이즈(백색잡음)만 랜덤하게 추가하는 방법입니다.
<img src="/assets/images/post_images//white_noise_injection.png" alt="White noise injection" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">white_noise_injection</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">pct</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="s">"""
    inputs
        series
            a pandas Series, must have DatetimeIndex
        pct
            a percentage of noise injection, within interval [0.0, 1.0] (default 0.1)
    output
        series
            a pandas Series with DatetimeIndex
    """</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">series</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">__filter_out_1sigma</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">mu</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">arr</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">arr</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">arr</span><span class="p">[((</span><span class="n">mu</span> <span class="o">-</span> <span class="n">sigma</span><span class="p">)</span> <span class="o">&lt;=</span> <span class="n">arr</span><span class="p">)</span> <span class="o">&amp;</span> <span class="p">(</span><span class="n">arr</span> <span class="o">&lt;=</span> <span class="p">(</span><span class="n">mu</span> <span class="o">+</span> <span class="n">sigma</span><span class="p">))]</span>
    <span class="n">sr_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sr_len</span><span class="p">)</span>
    <span class="n">_dif</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">temp</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">dif</span> <span class="o">=</span> <span class="n">__filter_out_1sigma</span><span class="p">(</span><span class="n">_dif</span><span class="p">)</span>
    <span class="n">_mean</span><span class="p">,</span> <span class="n">_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dif</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">dif</span><span class="p">)</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">sr_len</span> <span class="o">*</span> <span class="n">pct</span><span class="p">))</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">_mean</span><span class="p">,</span> <span class="n">_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">))[</span><span class="n">pos</span><span class="p">]</span>
    <span class="n">temp</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">temp</span>
</code></pre></div></div>
<h4 id="outlier-injection">Outlier injection</h4>
<!-- 간단 설명, 코드, 시각화 -->
<p>Outlier injection은 원본 데이터에 임의로 이상치(outlier)를 추가하는 방법입니다. 이상 탐지를 위한 데이터를 임의로 생성 및 모델 학습에 활용할 수 있습니다.
<img src="/assets/images/post_images//outlier_injection.png" alt="Outlier injection" /></p>
<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">outlier_injection</span><span class="p">(</span><span class="n">series</span><span class="p">,</span> <span class="n">pct</span><span class="o">=</span><span class="mf">0.005</span><span class="p">):</span>
    <span class="s">"""
    inputs
        series
            a pandas Series, must have DatetimeIndex
        pct
            a percentage of noise injection, within interval [0.0, 1.0] (default 0.005)
    output
        series
            a pandas Series with DatetimeIndex
    """</span>
    <span class="n">temp</span> <span class="o">=</span> <span class="n">series</span><span class="p">.</span><span class="n">copy</span><span class="p">()</span>
    <span class="k">def</span> <span class="nf">__filter_outlier</span><span class="p">(</span><span class="n">arr</span><span class="p">):</span>
        <span class="n">outlier</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">arr</span><span class="p">,</span> <span class="mf">0.995</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">arr</span><span class="p">[(</span><span class="n">arr</span><span class="o">&gt;=</span><span class="n">outlier</span><span class="p">)]</span>
    <span class="n">sr_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">arange</span><span class="p">(</span><span class="n">sr_len</span><span class="p">)</span>
    <span class="n">_dif</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">diff</span><span class="p">(</span><span class="n">temp</span><span class="p">.</span><span class="n">values</span><span class="p">)</span>
    <span class="n">dif</span> <span class="o">=</span> <span class="n">__filter_outlier</span><span class="p">(</span><span class="n">_dif</span><span class="p">)</span>
    <span class="n">_mean</span><span class="p">,</span> <span class="n">_std</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dif</span><span class="p">),</span> <span class="n">np</span><span class="p">.</span><span class="n">std</span><span class="p">(</span><span class="n">dif</span><span class="p">)</span>
    <span class="n">pos</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">choice</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="nb">int</span><span class="p">(</span><span class="n">sr_len</span> <span class="o">*</span> <span class="n">pct</span><span class="p">))</span>
    <span class="n">noise</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">random</span><span class="p">.</span><span class="n">normal</span><span class="p">(</span><span class="n">_mean</span><span class="p">,</span> <span class="n">_std</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="nb">len</span><span class="p">(</span><span class="n">temp</span><span class="p">))[</span><span class="n">pos</span><span class="p">]</span>
    <span class="n">temp</span><span class="p">.</span><span class="n">iloc</span><span class="p">[</span><span class="n">pos</span><span class="p">]</span> <span class="o">+=</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">temp</span>
</code></pre></div></div>
<!-- ## 데이터 증강 적용 -->
<!-- Use Cases of Time Series Data Augmentation -->
<!-- <!-- ### Improving performance-- -->
<!-- ### Small datasets -->
<!-- ### Imbalanced datasets -->

<!-- ## 데이터 증강 한계 및 과제
1. Data quality issues
여전히 햄스터인가?? -> 증강한 데이터가 원본 데이터의 특성을 유지하고 있는가??
2. Augmentation method selection
3. Computational complexity -->

<h2 id="마치며">마치며</h2>
<!-- 결론 -->
<p>머신러닝에서 <strong>데이터 증강</strong>은 <strong>기존 데이터를 변형시켜 새로운 데이터를 생성하는 기술</strong>입니다. 이를 통해 모델은 다양한 패턴을 학습하고 일반화 능력을 향상시킬 수 있습니다. 또한 <strong>데이터 증강</strong>은 일반적으로 <strong>데이터 크기를 증가</strong>시키는 데 사용되며, <strong>모델 과적합을 방지</strong>하고 <strong>데이터 불균형 문제를 해결</strong>하는 데도 유용합니다.</p>

<p>이번 글에서 <strong>시계열 데이터 증강기법</strong>으로 <strong>단순 변형 방법 3가지(Cropping, Warping, Injection)</strong>를 알아봤습니다. 증강기법을 직접 <u>코드로 구현해보고</u> <u>시각화 결과를 확인하며</u> 쉽게 이해할 수 있었습니다. 아직 실제 모델에 증강한 데이터를 활용해 본 적이 없어 효과를 직접 검증해보진 못했지만, 샘플 데이터로 증강을 진행해보면서 대부분 원본 데이터의 특성은 보존하고 있는 것을 확인했습니다. 이를 통해 시계열 데이터 증강이 모델 고도화에 충분히 유의미한 기술임을 확신할 수 있었습니다.</p>

<p>다음 글에서는 한 단계 나아가 생성 모델을 활용한 데이터 증강, 데이터 증강의 한계 및 과제는 어떤 것들이 있는지 알아보고 작성해보겠습니다.</p>]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Time series" /><summary type="html"><![CDATA[시작하며 시계열 데이터는 일정한 시간 간격으로 발생하는 시간에 따라 정렬된 데이터를 의미하며, 오늘날 제조, 금융, 의료 등 다양한 분야에서 발생하고 있습니다. 그리고 현업에서 머신러닝(회귀, 분류)을 위한 시계열 데이터는 다른 분야(이미지, 텍스트 등)와 같은 목적으로 활용됩니다.]]></summary></entry><entry><title type="html">K-nearest neighbor</title><link href="http://localhost:4000/machine%20learning/k_nearest_neighbor/" rel="alternate" type="text/html" title="K-nearest neighbor" /><published>2023-02-04T00:00:00+09:00</published><updated>2023-02-04T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/k_nearest_neighbor</id><content type="html" xml:base="http://localhost:4000/machine%20learning/k_nearest_neighbor/"><![CDATA[<p><br />
KNN(K-최근접 이웃, K-Nearest Neighbor)은 직관적이고 간단한 방법에 비해 좋은 성능을 보여주어 종종 사용되는 머신러닝 알고리즘이다. 대부분의 머신러닝 알고리즘은 훈련데이터를 통해 모델을 생성하는 방식이라면, <strong>KNN은 하나하나의 데이터 값을 통해 학습을 시행하고 따로 모델을 생성하지 않는 비모수 방식이라는 특징이 있다.</strong> KNN 알고리즘의 작동방식에 대해 알아보기전 KNN의 특징인 비모수 방식에 대해 짚고 넘어가보자.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/117812408-b5d1cd00-b29c-11eb-8b84-276ff99e144a.PNG" alt="knn3" /></p>

<p><strong>모수 방식과 비모수 방식의 가장 큰 차이는 확률분포의 개념을 활용하느냐의 차이이다.</strong> 예를 들어, 모수 방식인 선형 회귀는 두 변수 사이의 관계를 표현하는 것인데 이때 생기는 오차는 정규분포, 즉 확률분포로 정규분포를 따르기 때문에 확률분포의 개념을 활용한다. 반대로 비모수 방식은 두 변수 사이의 관계를 표현할때 생기는 오차는 정규분포를 따르지 않는다고 할 수 있다. 즉, 비모수 방식은 오차가 특정 분포를 이루고 있는게 아니라 의미없이 오차들이 골고루 분포하고 있는 것이기 때문에 확률분포의 개념을 활용하지 않는 것이다.</p>
<blockquote>
  <p>모수 방식과 비모수 방식은 선형, 비선형과 관련있다.<br />
모수 방식은 변수가 선형 관계일때 활용하고, 비모수 방식은 변수가 비선형 관계일때 활용하는 것이
적합한 방식이다.</p>
</blockquote>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/117812472-c7b37000-b29c-11eb-873e-860073605312.PNG" alt="knn1" /></p>

<p>KNN은 대표적인 비모수 방식으로 데이터 간의 거리를 활용해 알고리즘이 작동한다. 만약 임의로 지정한 k=3이라면 (일반적으로 k는 홀수를 사용, 짝수의 k는 동점을 초래하기 때문) 새로 들어온 데이터에서 거리가 가까운 3개의 데이터를 통해 분류를 시행한다. <strong>모델을 별도로 생성하지 않는 알고리즘의 특성상 결정 경계(Decision Boundary)가 존재하지 않으며, 새로운 데이터가 주어지면 학습을 통해 새로 모델을 생성할 필요없이 새로운 데이터만 추가하여 분류한다.</strong> 그리고 거리를 기반으로 분류하여 이상치, 노이즈에 크게 영향을 받지 않는다는 장점도 있다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/117812502-d1d56e80-b29c-11eb-9d41-311ed7bdf352.PNG" alt="knn2" /></p>

<p>KNN은 유클리드 거리와 마할라노비스 거리 등 다양한 계산방식을 통해 거리 측정이 가능하다. 유클리드 거리는 흔히 알고있는 피타고라스 정리에 기초해서 거리를 구하는 방식이고, 마할라노비스 거리는 각 특성의 분포를 파악하고 실제로 같은 거리라도 특성마다 다르게 측정하는 방식이다. 거리를 측정하기 위한 특성으로 x, y가 있다고 가정해보자. <strong>특성 x의 분산이 특성 y보다 크다면 x의 분포가 더욱 퍼져있을것이고, 이때 분산을 거리의 기준으로 보면 특성 x에서의 거리는 동일한 거리라도 특성 y에서 보다 더 짧다고 볼 수 있다.</strong> 이처럼 특성마다 분포가 다르기 때문에 거리의 스케일이 다를것이고 이를 동일하게 하기 위해 마할라노비스 거리를 활용한다.</p>

<blockquote>
  <p>차원의 저주 : 유클리드 거리가 고차원에서 도움이 되지 않음,
차원의 증가로 인한 데이터 간 거리 증가, PCA활용</p>
</blockquote>

<p><br /></p>

<h6 id="결론">결론</h6>
<ul>
  <li>KNN은 특정한 모델을 생성하지 않고 데이터를 통해 학습을 시행하는 비모수 방식이다.</li>
  <li>거리를 기반 알고리즘이기 때문에 노이즈에 크게 영향을 받지 않는다.</li>
  <li>거리 측정을 위한 방법으로 크게 유클리드 거리와 마할라노비스 거리가 있다.</li>
</ul>]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Model" /><category term="Backlog" /><summary type="html"><![CDATA[KNN(K-최근접 이웃, K-Nearest Neighbor)은 직관적이고 간단한 방법에 비해 좋은 성능을 보여주어 종종 사용되는 머신러닝 알고리즘이다. 대부분의 머신러닝 알고리즘은 훈련데이터를 통해 모델을 생성하는 방식이라면, KNN은 하나하나의 데이터 값을 통해 학습을 시행하고 따로 모델을 생성하지 않는 비모수 방식이라는 특징이 있다. KNN 알고리즘의 작동방식에 대해 알아보기전 KNN의 특징인 비모수 방식에 대해 짚고 넘어가보자.]]></summary></entry><entry><title type="html">Overfitting</title><link href="http://localhost:4000/machine%20learning/overfitting/" rel="alternate" type="text/html" title="Overfitting" /><published>2023-02-04T00:00:00+09:00</published><updated>2023-02-04T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/overfitting</id><content type="html" xml:base="http://localhost:4000/machine%20learning/overfitting/"><![CDATA[<p><br />
머신러닝에서 훈련 데이터로 모델을 생성하고 테스트 데이터에 적용해보면 훈련 데이터에서 좋았던 성능이 테스트 데이터에서 현저히 떨어지는 경우가 자주 발생한다. 테스트 데이터에 문제가 있거나 훈련 데이터를 통해 생성한 모델에 문제가 있어 성능에 차이가 생기는 것이다. 테스트 데이터에 문제가 있는 경우는 학습시 발생하는 것이 아니라 모델링과 관계없는 순수한 데이터의 문제이지만, 훈련 데이터로 생성한 모델은 모델링과 직접적인 관련이 있다. <strong>훈련 데이터를 과하게 학습하여 모델이 훈련 데이터에 최적화되어 테스트 데이터에서의 성능이 떨어지는 것인데 이를 과적합(Overfitting)이라 한다.</strong> 과적합은 머신러닝 과정에서 발생하는 대표적인 문제인데, 과적합을 발생시키지 않으면서 가능한 최대의 학습을 시키는 것이 머신러닝의 궁극적 목표이다.
<br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/118114243-4980d600-b422-11eb-878f-4425574acc92.PNG" alt="over1" /></p>

<p>회귀(Regression)에서 과적합을 보면, 회귀식이 모든 데이터에 피팅되어 구불해진 회귀식의 모습을 볼 수 있다. 만약 다른 이상치가 더 있다면 회귀식은 더욱 휘어진 형태로 과적합 될것이다. 그리고 분류(Classification)에서는 결정경계(Decision Boundary)가 오분류를 발생시키지 않기 위해 모든 데이터에 피팅 된 모습이다. 회귀에서는 회귀식이 유연한 형태였다면, 분류에서는 결정경계가 유연한 형태로 과적합이 발생한다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/118114281-556c9800-b422-11eb-8d4d-47d5486bf86f.PNG" alt="over2" /></p>

<p>과적합은 분산(Variance), 편향(Bias)과도 연관성이 있다. 위의 이미지를 보면 분산이 크고 편향이 작을 때
전형적인 과적합의 형태를 띄고있는데, <strong>분산이 크면 예측 분포가 큰 복잡한 모델을 만들어 과적합이 발생하고 편향이 크면 예측이 계속해서 틀리는 단순한 모델을 만들어 과소적합이 발생한다.</strong></p>
<blockquote>
  <p>분산과 편향은 트레이드 오프(Trade-off) 관계로
분산이 커지면 편향이 작아지고 분산이 작아지면 편향이 커지게 된다.</p>
</blockquote>

<p><br /></p>

<p>그렇다면 어떤 방법으로 과적합을 해결하고 모델의 성능을 높일 수 있을까?
과적합을 해결하는 가장 간단한 방법은 훈련 데이터를 추가하는 것이다. 새로운 훈련 데이터가 생기면 기존의 데이터들의 영향력이 조금 줄어들어 과적합 해결이 가능할것이다. <strong>즉, 모델의 전체 영향력을 기존 데이터가 나눠 갖고 새로운 데이터의 추가로 전체적인 영향력이 줄어들어 과적합이 사라지는 것이다.</strong> 다음의 방법으로 데이터 차원을 감소시키는 방법이 있다. SVM 모델을 예로, 커널 트릭을 활용해 데이터의 차원을 증가시켜 2차원에서 분류 불가능한 데이터를 3차원에서 분류할 수 있는데 이때 3차원에서 생성한 모델을 2차원으로 변형시키면 결정 경계가 과적합된 경우와 비슷한 형태를 보이게 된다. <strong>반대의 경우를 생각해보면 과적합 된 모델의 데이터 차원을 감소시키면 결정 경계가 완만해져 과적합 해결이 가능하다.</strong>
마지막 방법으로 규제(Regularization)를 통해 모델 복잡도를 감소시켜 과적합이 일어나지 않게 하는 것으로, <strong>위 방법들은 모델을 훈련하기 전에 시행하는 방식이라면 규제는 학습을 진행하면서 과적합을 제어하는 방식이다.</strong> 규제는 크게 계수의 절대값에 비례하여 패널티를 주는 Lasso(L1)과 계수의 제곱에 비례하여 패널티를 주는 Ridge(L2) 방식이 있다. 두 방식 모두 비용 함수(Cost Function)에 패널티를 추가해 기존의 비용(Cost)이 증가하여 학습을 최소화하게 된다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/118114663-dcba0b80-b422-11eb-8c42-cd202bab02c4.PNG" alt="over5" /></p>

<p>수식을 보면, 기존의 Cost Function에 각각의 패널티가 더해진 것을 볼 수 있다. <strong>이때 패널티는 가중치(w)에 학습률(Learning Rate)를 곱한 값으로 Cost Function에 포함되어 학습을 하면서 가중치 스스로 값이 커지는 것을 제어한다.</strong> 가중치가 큰 값을 갖지 못하기 때문에 회귀식에서 과적합으로 휘어진 식이 과적합 되지 못하고 직선형태로 변하게 되는 것이다. 모델 생성 시 타원의 중앙에서 Cost Function 최소가 되지만, 규제를 통해 타원의 중앙에 가장 근접한 가중치를 찾게 된다. 이때의 Cost Function은 타원의 중앙값이 아니라 최소가 되는 지점이 아니라 생각할수도 있으나 실제 데이터에서는 모델에 규제가 적용되어 더 좋은 효과를 보여주는 것이다.</p>

<p><br /></p>

<h5 id="결론">결론</h5>
<ul>
  <li>과적합(Overfitting)은 훈련 데이터를 과하게 학습하여 모델이 훈련 데이터에 최적화 된 것을 말한다.</li>
  <li>과적합은 분산(Variance), 편향(Bias)과도 관련있으며, 분산이 크면 예측 분포가 큰 복잡한 모델을 만들어 과적합이 발생한다.</li>
  <li>규제(Regularization)는 비용 함수(Cost Function)에 패널티를 추가해 과적합을 방지한다.</li>
</ul>

<!--
절편을 더해주는 것
cost 최소화가 목표
규제가 없는 비용 함수로 훈련한 모델에 비해 가중치 값을 아주 작게 만드는 효과를 냅니다
비용함수의 증가
-->]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Backlog" /><summary type="html"><![CDATA[머신러닝에서 훈련 데이터로 모델을 생성하고 테스트 데이터에 적용해보면 훈련 데이터에서 좋았던 성능이 테스트 데이터에서 현저히 떨어지는 경우가 자주 발생한다. 테스트 데이터에 문제가 있거나 훈련 데이터를 통해 생성한 모델에 문제가 있어 성능에 차이가 생기는 것이다. 테스트 데이터에 문제가 있는 경우는 학습시 발생하는 것이 아니라 모델링과 관계없는 순수한 데이터의 문제이지만, 훈련 데이터로 생성한 모델은 모델링과 직접적인 관련이 있다. 훈련 데이터를 과하게 학습하여 모델이 훈련 데이터에 최적화되어 테스트 데이터에서의 성능이 떨어지는 것인데 이를 과적합(Overfitting)이라 한다. 과적합은 머신러닝 과정에서 발생하는 대표적인 문제인데, 과적합을 발생시키지 않으면서 가능한 최대의 학습을 시키는 것이 머신러닝의 궁극적 목표이다.]]></summary></entry><entry><title type="html">Unsupervised learning</title><link href="http://localhost:4000/machine%20learning/unsupervised_learning/" rel="alternate" type="text/html" title="Unsupervised learning" /><published>2023-02-04T00:00:00+09:00</published><updated>2023-02-04T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/unsupervised_learning</id><content type="html" xml:base="http://localhost:4000/machine%20learning/unsupervised_learning/"><![CDATA[<p><br />
머신러닝은 크게 지도학습(Supervised Learning), 비지도학습(Unsupervised Learning), 강화학습(Reinforcement Learning)으로 나뉜다. 지도학습과 강화학습은 입력(Input)에 대한 결과(Output)를 주고 학습하는 방식이지만 비지도학습은 입력에 대한 결과가 없이 입력만으로 학습을 진행한다. 비지도학습을 시험에 빗대면 문제에 정답이 없는 논술형 시험과 비슷하다고 생각하는데 정답이 없기 때문에 정답을 판단하는 기준이 모호하고 결과는 해석하기 나름이라는 공통점이 있다.</p>

<p><img src="https://user-images.githubusercontent.com/82218035/119490037-5218cb00-bd97-11eb-8b2b-7e373249f9bd.PNG" alt="unsupervised learning1" /></p>

<p><br /></p>

<p>비지도학습의 가장 대표적인 기법에는 군집화(Clustering)가 있다. 군집화는 논술형 시험과 유사하다. 논술형 시험은 정해진 결론이 없고 유사한 결론이 있기 때문에 비슷한 결론끼리 묶음 처리가 가능할 것이다. 이처럼 정해진 답은 없지만 유사한 답을 묶어 처리하는 방식이 군집화의 원리이다. 그리고 군집화를 시행하고 각각의 군집에 속한 사람들의 특징을 파악하면 군집 내부에서 새로운 특징 도출이 가능한데 만약 답안으로 긍정적인 결론을 지은 사람들을 모아놓고 다른 특징을 파악해보면 대부분 사람들은 밝은 성격을 갖고 있다는 새로운 특징을 찾을 수 있을 것이다.</p>

<blockquote>
  <p>군집화는 지도학습의 분류(Classification)와 공통점이 있다. 두 방식 모두 출력값이 분류 가능한 이산형 값으로 나와 데이터의 분류가 가능하다는 것이다. 그렇다면 비지도학습에서 지도학습의 회귀(Regression) 문제와 같이 연속형 값으로 예측은 불가능한가?</p>
</blockquote>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/119490066-5a710600-bd97-11eb-8861-a19c8c33e406.PNG" alt="unsupervised learning2" /></p>

<p>비지도학습은 차원 축소(Dimensionality Reduction)로 데이터의 주요 특징을 추출하고 변환 가능하다. 차원 축소 기법인 주성분 분석(PCA, Principal Component Analysis)을 실시하여 차원을 축소하고 복원하는 과정에서 데이터의 주요 특징만 남는 원리를 활용해 비지도 학습의 이상치 탐지(Unsupervised Anomaly Detection)가 가능하다.</p>

<p><img src="https://user-images.githubusercontent.com/82218035/119490097-68bf2200-bd97-11eb-842b-0275e107fe2f.PNG" alt="unsupervised learning3" /></p>

<p>이때 입력 데이터의 특징을 추출하기 위해 오토인코더(Autoencoder)를 사용하는데 정상 데이터가 오토인코더를 거치면 정상 데이터가 출력되어 Input과 Output에 차이가 없는 반면, 비정상 데이터가 오토인코더를 거치면 정상 데이터가 출력되어 Input과 Output에 차이가 발생해 비정상 데이터가 구분 가능해진다.</p>

<p><br /></p>

<p>인공지능의 목적은 비지도학습이 가능한 인공지능을 만드는 것이라 생각한다. 기계가 인간처럼 스스로 판단하고 행동하도록 만들기위해 머신러닝의 지도학습은 정답을 보면서 학습하지만, 비지도학습은 정답을 모르고 계속해서 학습을 진행하기 때문에 처음 맞닥뜨린 상황이더라도 스스로 판단을 한다. 그리고 기계가 처음 접하는 상황에 자신만의 판단을 만든다면, 사람처럼 성격과 인격을 갖춘 인공지능이 구현가능할 것이다. 이처럼 인공지능의 최종 목표는 비지도학습을 통해서 완성되고 구현될것이라 본다.</p>

<p><br /></p>

<h5 id="결론">결론</h5>
<ul>
  <li>비지도학습은 입력에 대한 결과가 없이 입력만으로 학습을 진행한다.</li>
  <li>비지도학습은 차원 축소로 데이터의 주요 특징을 추출하고 변환 가능하다.</li>
  <li>인공지능의 완전한 모습은 비지도학습으로 구현될것이다.</li>
</ul>]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Backlog" /><summary type="html"><![CDATA[머신러닝은 크게 지도학습(Supervised Learning), 비지도학습(Unsupervised Learning), 강화학습(Reinforcement Learning)으로 나뉜다. 지도학습과 강화학습은 입력(Input)에 대한 결과(Output)를 주고 학습하는 방식이지만 비지도학습은 입력에 대한 결과가 없이 입력만으로 학습을 진행한다. 비지도학습을 시험에 빗대면 문제에 정답이 없는 논술형 시험과 비슷하다고 생각하는데 정답이 없기 때문에 정답을 판단하는 기준이 모호하고 결과는 해석하기 나름이라는 공통점이 있다.]]></summary></entry><entry><title type="html">Ensemble</title><link href="http://localhost:4000/machine%20learning/ensemble/" rel="alternate" type="text/html" title="Ensemble" /><published>2023-02-04T00:00:00+09:00</published><updated>2023-02-04T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/ensemble</id><content type="html" xml:base="http://localhost:4000/machine%20learning/ensemble/"><![CDATA[<p><br />
앙상블(Ensemble)은 프랑스어로 단어 자체의 의미가 조화, 통일을 뜻한다. 머신러닝에서 앙상블은
여러 모델을 조화시켜 하나의 새로운 모델을 만드는 것을 의미하며, 여러 모델의 조합으로 만들어졌기 때문에 일반적으로 더욱 좋은 성능을 보여준다. 앙상블의 기본 원리를 알면 앙상블을 왜 사용하는지 이해가능하다. 데이터의 크기가 작다면, 한가지 기법만 사용해 하나의 모델만 생성하여 적용해도 모델이 좋은 성능을 보여줄것이다. 그러나 현재 대부분의 데이터는 크기가 크고 다양해서 하나의 모델만만으로 데이터를 충분히 설명하기 어렵다. 이러한 문제점을 해결하기 위해 모델 각각의 특징을 결합한 새로운 모델을 생성하는 앙상블 기법을 사용하는 것이다.</p>

<p><img src="https://user-images.githubusercontent.com/82218035/120065878-93b6b800-c0ae-11eb-8ee7-9df1c9f8b80d.png" alt="KakaoTalk_Photo_2021-05-29-18-32-54" /></p>

<p>실생활을 예로 축구팀에 빗대어보자. 팀에 공격수만 있다면, 공격은 잘되지만 수비수가 없어 골을 잘 먹혀 대회에서 좋은 성적을 거두지 못할것이다. 공격수와 수비수가 둘다 있는 공격과 수비에 밸런스를 맞춘 조화로운 팀을 만든다면 한가지에 특출난 팀보다 우수한 성적을 거둘것이다. 축구선수 한명을 모델로 보고 팀의 성적을 모델의 성능이라 가정해보면, 축구는 혼자 팀을 하는 것보다 선수가 많을수록, 공격수만 있는팀 보다 다양한 포지션의 선수가 있는 팀일수록 좋은 성적을 거둘것이다. 앙상블도 마찬가지로 모델은 많을수록, 다양한 모델이 있을수록 좋은 성능을 보여주는 것이다.</p>

<p><br />
앙상블에는 보팅(Voting), 배깅(Bagging), 부스팅(Boosting) 방식이 있다. 보팅과 배깅은 완성된 모델을 결합하는 방식이라면, 부스팅은 모델을 수정해가면서 계속해서 새로운 모델을 만드는 방식이다. 보팅부터 살펴보면, 보팅은 크게 하드 보팅(Hard Voting)과 소프트 보팅(Soft Voting)으로 나뉜다. 두 방법 모두 모델마다 투표권을 갖고 결과에 대한 투표권을 행사하여 많은 득표를 받은 값을 최종 값으로 선정하는 투표 방식의 앙상블 기법으로 하드 보팅은 각 모델의 결과값을 종합하여 많이 나온 값을 최종 값으로 정하는 방식이고 소프트 보팅은 모델의 결과값이 나올 각 확률을 결합하여 최종 값을 정하는 방식이다.</p>

<p><br />
<img width="648" alt="스크린샷 2021-05-29 오후 6 36 59" src="https://user-images.githubusercontent.com/82218035/120065891-a9c47880-c0ae-11eb-9041-dc9c8fa8f61d.png" /></p>

<p>하드 보팅은 말그대로 투표권을 분산 없이 하나의 값에 부여하여 비슷한 확률 값을 갖는 결과는 무시해버리는 경우가 생겨 소프트 보팅보다 유연성 측면에서 떨어진다. 앙상블 원리 자체가 여러 모델을 결합하고자 하는 것인데, 하드 보팅은 모델을 결합한다기보다 결과값을 취합하는 방식같다는 개인적인 느낌이 들어 앙상블의 취지에 부합하지 않다는 생각이 든다. 그래서 일반적으로 하드 보팅보다는 소프트 보팅을 선호하는 경향이 있고 성능적인 측면에서도 소프트 보팅이 더 좋은 성능을 보여주기도 한다.</p>

<p><br />
<img src="https://user-images.githubusercontent.com/82218035/120065883-97e2d580-c0ae-11eb-887a-7b20bb4503d9.png" alt="KakaoTalk_Photo_2021-05-29-18-32-49" /></p>

<p>배깅은 보팅과 비슷한 방식으로 둘은 자주 비교된다. 보팅은 서로 다른 알고리즘을 활용한 모델을 결합하는 것이고, 배깅은 같은 알고리즘을 활용한 모델에 데이터를 다르게 적용하여 생성된 모델을 결합하는 방식이다. 배깅과 보팅은 같은 알고리즘을 활용하느냐, 다른 데이터를 활용하느냐가 두 방식의 차이점이라 생각한다.<br />
마지막으로 부스팅은 앞의 보팅, 배깅과 다르게 모델을 결합한다기보다 계속적으로 모델을 업데이트하는 방식이다. 생성한 모델을 검증 데이터에 적용해 오분류 데이터를 찾고 오분류 데이터에 가중치를 부여해 오분류 데이터를 잘 분류하도록 다음 모델을 업데이트 한다. 이런 과정을 여러번 반복하다보면 최종적으로 생성한 모델은 에러와 오차가 줄어드는 것이다.</p>

<p>앙상블은 현재시점에서 정형데이터에 좋은 성능을 보여주고있다. 데이터가 이미지나, 텍스트 등의 복잡한 데이터가 아니라면 머신러닝의 단순한 알고리즘을 앙상블시키면 충분한 성능을 보여준다는 것이다. 아직까지 관계형데이터베이스(RDB)의 정형데이터를 자주 접하고 있기 때문에 앙상블을 잘 활용한다면 좋은 모델을 만들 수 있다.</p>
<blockquote>
  <p>딥러닝 모델도 앙상블을 통해 성능을 높이는 방식도 가능하다.</p>
</blockquote>

<p><br /></p>

<h6 id="결론">결론</h6>
<ul>
  <li>앙상블은 여러 모델을 조화시켜 하나의 새로운 모델을 만드는 것을 의미한다.</li>
  <li>모델은 많을수록, 다양한 모델이 있을수록 좋은 성능을 보여준다.</li>
  <li>앙상블 기법은 모델 각각의 특징을 결합한 새로운 모델을 생성한다.</li>
</ul>]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Backlog" /><summary type="html"><![CDATA[앙상블(Ensemble)은 프랑스어로 단어 자체의 의미가 조화, 통일을 뜻한다. 머신러닝에서 앙상블은 여러 모델을 조화시켜 하나의 새로운 모델을 만드는 것을 의미하며, 여러 모델의 조합으로 만들어졌기 때문에 일반적으로 더욱 좋은 성능을 보여준다. 앙상블의 기본 원리를 알면 앙상블을 왜 사용하는지 이해가능하다. 데이터의 크기가 작다면, 한가지 기법만 사용해 하나의 모델만 생성하여 적용해도 모델이 좋은 성능을 보여줄것이다. 그러나 현재 대부분의 데이터는 크기가 크고 다양해서 하나의 모델만만으로 데이터를 충분히 설명하기 어렵다. 이러한 문제점을 해결하기 위해 모델 각각의 특징을 결합한 새로운 모델을 생성하는 앙상블 기법을 사용하는 것이다.]]></summary></entry><entry><title type="html">Support vector machine</title><link href="http://localhost:4000/machine%20learning/support_vector_machine/" rel="alternate" type="text/html" title="Support vector machine" /><published>2023-02-04T00:00:00+09:00</published><updated>2023-02-04T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/support_vector_machine</id><content type="html" xml:base="http://localhost:4000/machine%20learning/support_vector_machine/"><![CDATA[<p><br />
머신러닝의 지도학습 기법으로 주로 분류에 자주 사용되는 SVM에 대하여 알아보고자 한다. SupportVectorMachine의 줄일말로 <strong>SupprotVector(서포트 벡터)와 Hyperplane(초평면)을 이용해 DecisionBoundary(결정 경계)를 정의하고 분류를 시행하는 알고리즘</strong>이다.</p>

<blockquote>
  <p>DecisionBoundry<br />
두 개의 계층을 가지고 있는 통계적인 분류 문제에서,
결정 경계는 기본 벡터공간을 각 클래스에 대하여 하나씩 두 개의 집합으로 나누는 초표면이다. 분류기는 결정 경계의 한쪽에 있는 모든 점을 한 클래스, 다른 한쪽에 있는 모든 점을 다른 클래스에 속하는 것으로 분류한다.</p>
</blockquote>

<p><strong>SVM은 최적의 결정 경계를 찾는 것이 목적인데</strong> 여기서 최적의 결정 경계를 찾기 위해서는 Margin(마진)의 최대점을 찾아야한다. 마진은 결정 경계와 서포트 벡터 사이의 거리로 큰 값을 가질수록 분류 경계가 커짐을 뜻하기 때문에 좋은 분류 성능을 보여줄것이다.</p>

<p><img src="https://user-images.githubusercontent.com/82218035/116775479-252a0e80-aa9e-11eb-9e02-f1a8d75e6545.png" alt="KakaoTalk_Photo_2021-05-01-16-55-49" /></p>

<blockquote>
  <p>SVM 구성요소<br />
Hyperplane : 초평면으로 고차원의 결정 경계를 뜻하기도 한다.<br />
SupportVector : 결정 경계와 가까이 있는 데이터 포인터<br />
margin : 결정 경계와 서포트 벡터 사이의 거리</p>
</blockquote>

<p><br /></p>

<p><strong>SVM에서 결정 경계는 p차원의 데이터라면 p-1차원의 결정 경계가 만들어진다.</strong> 예를 들어, 3차원의 데이터는 2차원의 평면으로 경계가 만들어지고, 2차원의 데이터는 1차원의 벡터(선)으로 경계를 만든다. 이러한 결정 경계의 특징 때문에 초기에 결정 경계를 구하는 알고리즘은 선형 분류만 가능했었고, 비선형 분류가 불가능했었다. 하지만 <strong>데이터의 분포에 커널 트릭을 적용시켜 분포를 고차원으로 변형함으로써 비선형 분류도 가능하도록 만들었다.</strong></p>

<blockquote>
  <p>커널트릭<br />
저차원 공간을 고차원으로 매핑해주는 작업, 선형 분리가 불가능한 1차원 데이터를 2차원의 공간으로 매핑하여 선형분리가 가능하도록 한다.</p>
</blockquote>

<p><img src="https://user-images.githubusercontent.com/82218035/116775481-2824ff00-aa9e-11eb-8c6f-dd261c5a964b.png" alt="KakaoTalk_Photo_2021-05-01-16-55-57" /></p>

<p>위의 그림은 커널트릭을 활용해 선형분리가 불가능한 1차원의 데이터에 2차원의 데이터를 추가함으로써 선형분리가 가능하도록 하는것이다. <strong>그렇다면 커널트릭을 적용해 저차원의 데이터를 고차원으로 매핑하여 시행하는 분류의 성능은 선형분류보다 정확한 성능을 보여주는 것인가?</strong> 하는 의문이 생긴다. 2차원의 데이터를 1차원의 벡터로 선형 분류를 시행하는 방법과 2차원의 데이터를 3차원으로 변형시켜 2차원의 초평면으로 비선형 분류를 시행하는 방법중 어떤 것이 더 높은 정확도를 보여주는지 파악해보았다.</p>

<p><img width="713" alt="스크린샷 2021-05-01 오후 4 26 48" src="https://user-images.githubusercontent.com/82218035/116775482-2ce9b300-aa9e-11eb-8767-52a9ac4119b6.png" />
<br />
<br />
<img width="713" alt="스크린샷 2021-05-01 오후 4 27 03" src="https://user-images.githubusercontent.com/82218035/116775484-2e1ae000-aa9e-11eb-8caa-5ca9bf18d7f1.png" /></p>

<p><br />
<br /></p>

<p>분류를 시행해 본 결과, <strong>결론적으로 고차원 공간으로 변형시킨 방법이 데이터를 더 잘 분류했다.</strong> 선형분리가 가능한 데이터에서 커널트릭을 활용한 분류가 마진이 더 컸으며, 비선형 분류를 해야하는 데이터도 선형분리가 불가능한 단점을 유연한 결정 경계로 데이터를 분류해 커널트릭을 활용한 분류가 항상 더 좋은 분류라 생각할수도있다. <strong>그러나 커넡트릭을 활용한 분류는 한가지 큰 문제점이 있다. 머신러닝에서 자주 발생하는 문제점인 OverFitting(과적합)을 초래한다는 것인데, 데이터 차원을 증가시켜 고차원에서 학습시킨 모델은 다시 본래의 차원에서 과적합이 발생하기 때문이다.</strong></p>

<blockquote>
  <p>머신러닝에서 모델이 유연할수록 과적합이 쉽게 발생하는 것은 당연하다.
그리고 유연함의 정도에 따라 과적합까지의 임계값은 존재하고, 임계값 내에서의
최대치를 찾는 것이 가장 좋은 모델이다.</p>
</blockquote>

<p><br /></p>

<p><strong>SVM 모델에서 중요한 HyperParameter(하이퍼파라미터)로 kernel(커널), C(규제), gamma(커널계수)가 있다.</strong> 앞에서 kernel에 대해서는 확인해보았기 때문에 규제와 커널계수인 C, gamma를 조정하여 모델이 데이터를 어떻게 분류하는지 살펴보자.</p>

<p><img width="699" alt="스크린샷 2021-05-01 오후 4 53 55" src="https://user-images.githubusercontent.com/82218035/116775485-31ae6700-aa9e-11eb-9954-b076d2cc9b6d.png" /></p>

<p><br />
<br /></p>

<p>C는 규제의 의미로 큰 값일수록 강력한 규제를 뜻하고, 규제가 강할수록 과적합이 일어남을 볼 수 있다. gamma는 커널 계수로 클수록 결정 경계와 서포트 벡터 사이의 거리가 멀어지고 과적합이 일어난다. C와 gamma는 클수록 과적합이 발생하기 때문에 <strong>조절을 통해 최적의 값을 찾는다면 좋은 분류모델을 만들 수 있을 것이다.</strong></p>

<p><br /></p>

<h6 id="결론">결론</h6>
<ul>
  <li>SVM은 SupprotVector(서포트 벡터)와 Hyperplane(초평면)을 이용해 DecisionBoundary(결정 경계)를 정의하고 분류를 시행하는 알고리즘이다.</li>
  <li>SVM에서 선형분리는 p개의 변수, p차원을 p-1 차원으로 분리하는 것이고, 비선형분리는 p개의 변수, p차원을 p+1 차원으로 분리하는 것이다.</li>
  <li>kernel, C, gamma의 조절을 통해 과적합이 일어나지 않는 최적의 값을 찾아 우수한 성능의 모델을 만들 수 있다.</li>
</ul>

<!--

SVM에서 중요한 하이퍼 파라미터로는
kernel(kernel type) -
C(규제) - 클수록 강력한 규제, 클수록 과적합
gamma(커널 계수, 가우시안 커널 폭을 제어, 데이터 포인트 사이의 거리)
클수록 데이터 포인트 사이의 거리가 , 클수록 과적합


다시 본래의 차원에서 과적합을 초래할수있다는 것이다.

고차원 공간에서의 분류는 기존 공간에서의 분류 일반화의 오차를 상승시킨다는 것이 증명되었고,
그 오차의 상승 폭에 대한 임계값이 있다.
과적합까지의 임계값이 있다.



비선형분리

Hyperplane
p차원에서 p-1차원의 Hyperplane 생성

분류기는 고차원 특징 공간에서 선형 초평면이지만
기존 차원 공간에서는 비선형 초평면이다
초평면의 차원증가???
고차원 공간을 이용한 분류는 과적합을 초래하여
기존 공간에서 분류 일반화의 오차를 상승시킨다.
				(기존 공간에서 분류 오차를 증가시킨다.)
				(기존 공간에서의 오분류를 발생시킨다.)



선형분리
p개의 변수, p차원을 p-1 차원으로 분리
3차원을 2차원으로 분리, 2차원을 1차원으로 분리

비선형분리
p개의 변수, p차원을 p+1 차원으로 분리
2차원을 3차원으로 분리, 1차원을 2차원으로 분리

소프트벡터머신에 가장 근접한 값들만 활용해 서포트벡터를 그린다.


n개의 속성을 가진 데이터는 최소 n+1개의 서포트 벡터가 존재한다.


from sklearn.datasets.samples_generator import make_blobs
X, y = make_blobs(n_samples=40, centers=2, random_state=20)



option
	kernel
	C
	gamma


svm = SVC(kernel='rbf', random_state=1, gamma=0.10, C=10.0)
svm.fit(X_xor, y_xor)
plot_decision_regions(X_xor, y_xor, classifier=svm)
plt.legend(loc='upper left')
plt.tight_layout()
plt.show()

clf = svm.SVC(kernel='linear')
clf.fit(X_train, y_train)
plot_decision_function(X_train, y_train, X_test, y_test, clf)



하이퍼 파라미터별로 그래프 그려본 후
그리드 서치를 통한 최적의 하이퍼파라미터 설정


파라미터확인
class_weight_ : ndarray of shape (n_classes,)
classes_ : ndarray of shape (n_classes,)
coef_ : ndarray of shape (n_classes * (n_classes - 1) / 2, n_features)
dual_coef_ : ndarray of shape (n_classes -1, n_SV)
fit_status_ : int
intercept_ : ndarray of shape (n_classes * (n_classes - 1) / 2,)
support_ : ndarray of shape (n_SV)
support_vectors_ : ndarray of shape (n_SV, n_features)   #서포트벡터 위치 반환 scatter활용
n_support_ : ndarray of shape (n_classes,), dtype=int32  #서포트벡터 갯수


커널 활용한 데이터 선형 분리 시각화


결정경계 시각화
import mglearn

plt.figure(figsize=[10,8])
mglearn.plots.plot_2d_classification(model, X_train, cm='spring')
mglearn.discrete_scatter(X_train[:,0], X_train[:,1], y_train)
-->]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Model" /><category term="Backlog" /><summary type="html"><![CDATA[머신러닝의 지도학습 기법으로 주로 분류에 자주 사용되는 SVM에 대하여 알아보고자 한다. SupportVectorMachine의 줄일말로 SupprotVector(서포트 벡터)와 Hyperplane(초평면)을 이용해 DecisionBoundary(결정 경계)를 정의하고 분류를 시행하는 알고리즘이다.]]></summary></entry><entry><title type="html">Decision tree</title><link href="http://localhost:4000/machine%20learning/decision_tree/" rel="alternate" type="text/html" title="Decision tree" /><published>2023-02-04T00:00:00+09:00</published><updated>2023-02-04T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/decision_tree</id><content type="html" xml:base="http://localhost:4000/machine%20learning/decision_tree/"><![CDATA[<p><br />
머신러닝의 기법으로 결정 트리(Decision Tree)를 사용하여 예측 모델을 만드는 결정 트리 학습법(Decision Tree Learning)이 있다. 결정 트리는 의사 결정 규칙과 그에 따른 결과를 보기 쉽게 트리 구조로 나타내어 결과에 대한 원인 파악이 가능하다는 장점이 있다. 즉, 결정 트리의 규칙들이 어떤 기준으로 설정되었는지 살펴보고 분류 결과 확인이 같이 가능하기 때문에 <strong>분류 기준을 파악하기에 유용할것이다.</strong></p>

<p><img src="https://user-images.githubusercontent.com/82218035/117408108-690f8e80-af4a-11eb-9c24-948b0564476b.PNG" alt="decisiontree5" /></p>

<p><br /></p>

<p><strong>결정 트리의 종류로 분류트리(Classification Tree)와 회귀트리(Regression Tree)가 있는데</strong>, 두 방식 모두 특정한 기준을 정하고 그 순간의 최적의 결정을 하는 greedy 방식을 따른다. 분류트리는 Target변수(Y)가 범주형 변수인 경우 사용하는데 <strong>정보이득(InformationGain)이 최대가 되는 특성으로 데이터를 나누고 정보이득은 불순도(Impurity)를 통해 계산한다.</strong> 여기서 불순도는 타겟변수의 분류가 잘 되어 있을수록 0에 가까운 값이 나온다.</p>

<p><img src="https://user-images.githubusercontent.com/82218035/117397012-19bf6300-af36-11eb-87a2-91a13e913399.PNG" alt="decisiontree3" /></p>

<blockquote>
  <p>불순도
불순도를 쉽게 표현하자면 방의 어질러진 정도를 나타낸다고 표현한다.
불순도는 높게 나올수록 방이 어지럽다는 뜻이고, 분류가 이뤄지지 않았다고 이해할수있으며, 지표로는 지니계수(Gini Index)와, 엔트로피(Entropy)가 있다.</p>
</blockquote>

<p><strong>분류에서 정보이득이 최대가 된다면 부모 노드의 불순도와 자식 노드의 불순도의 차이가 최대임을 뜻하는데, 노드간의 불순도의 차이가 최대가 되어야 분류가 잘 이뤄졌음을 뜻하기 때문이다.</strong></p>

<blockquote>
  <p>정보이득과 불순도<br />
정보이득은 분류의 기준을 찾기 위한 노드 간의 불순도 차이이고, <br />
불순도는 분류의 기준을 설정하기 위한 노드 내부의 수치이다.</p>
</blockquote>

<p>회귀트리는 Target변수(Y)가 연속형 변수인 경우 사용하며, <strong>잔차 제곱합(RSS)을 최소화 시킬 수 있는 방향으로 분류 기준을 만드는 방식이다.</strong> 분류 트리와 크게 다르지 않으며 예측값이 범주형이 아니기 때문에 예측한 구간내의 평균값으로 예측값을 계산한다. 회귀트리를 회귀분석과 혼동 할 수 있는데, 회귀트리는 회귀 함수를 기반으로 하지 않고 트리 기반으로 예측을 진행하기 때문에 <strong>트리구조로 데이터를 구간화 시킨 후 나뉜 구간별로 회귀분석을 시행하는 방식이라 생각한다.</strong></p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/117397071-46737a80-af36-11eb-84cf-a886abc67779.PNG" alt="decisiontree4" /></p>

<p>위의 그림은 동일한 데이터를 각각 결정 트리와 서포트 벡터 머신(SVM)으로 분류를 진행한 결과이다. 두 분류 모델의 가장 큰 차이점을 결정 경계(Decision Boundary)로 확인 가능하다. <strong>결정 트리는 재귀적 분기 방식으로 한번에 하나의 특성(Feature)만 고려하여 분류를 진행하기 때문에 결정 경계가 모두 수직을 이루고 있으며,</strong> 서포트 벡터 머신은 모든 특성을 고려하여 초평면을 찾고 분류를 진행하기 때문에 결정 경계가 곡선형태로 나타난다. 이러한 결정 트리의 분류는 <strong>일차원 방식이라 과적합이 발생하지 않을 것이라 생각 할 수도 있지만, 일차원 분류가 여러 번 시행되어 분류 횟수의 마지노선을 정해주지 않으면 과적합이 쉽게 발생하는 단점이 있다.</strong> (그림의 분류 경계만 보더라도 하나의 값을 분류하기 위해 모델이 과적합된 모습을 볼 수 있다)
그렇다면 결정 트리는 어떤 데이터의 분류에 적용하는 것이 효과적일까?<br />
<strong>결정 트리는 독립변수와 종속변수 간의 관계가 비선형적인 관계를 나타낼때 좋은 모델을 만들 것이다.</strong> 독립변수의 값이 커질수록 결정 경계가 같이 변한다면 선형 형태의 분류가 적합하겠지만, 하나의 특성이 여러 구간으로 분리 된다면 단순한 비선형 형태의 분류인 결정 트리가 효과적일 것이다.</p>

<h5 id="결론">결론</h5>
<ul>
  <li>머신러닝의 기법인 결정 트리(Decision Tree)는 분리 규칙을 트리 구조로 만들어 모델 이해가 쉽다.</li>
  <li>분류트리는 정보이득(InformationGain)으로 데이터를 나누고 정보이득은 불순도(Impurity)를 통해 계산한다.</li>
  <li>결정 트리는 비선형적인 관계를 분류할때 효과적인 모델을 만든다.</li>
</ul>

<!--

**선형 모델을 만드는 것인가??
선형성 존재여부 -> 과적합 발생


문제점 (과적합)
발생원인
해결방법
가지치기


회귀 트리는 회귀 함수를 기반으로 하지 않고 결정 트리와 같이 트리기반으로 한다.
즉, 회귀를 위한 트리를 생성하고 이를 기반으로 회귀 예측을 하는 것이다.
회귀 트리의 경우 분류 트리와 크게 다르지 않다. 다만 리프 노드에서 예측 결정값을 만드는 과정에 차이가 있다.
분류 트리가 특정 클래스 레이블을 결정하는 것과는 달리
회귀 트리는 리프 노드에 속한 데이터 값의 평균값을 구해 회귀 예측값을 계산한다.


DecisionTree종류
분류트리,
정보 이득, 불순도(엔트로피, 지니)
**불순도개념


회귀트리
RSS(잔차 제곱합)


**선형 모델을 만드는 것인가??
선형성 존재여부 -> 과적합 발생
회귀트리도 특정 구간에 대한 회귀만 시행


문제점 (과적합)
발생원인
해결방법



RandomForest(배깅)
RandomForest란?
DecisionTree의 문제점 해결
-->]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Model" /><category term="Backlog" /><summary type="html"><![CDATA[머신러닝의 기법으로 결정 트리(Decision Tree)를 사용하여 예측 모델을 만드는 결정 트리 학습법(Decision Tree Learning)이 있다. 결정 트리는 의사 결정 규칙과 그에 따른 결과를 보기 쉽게 트리 구조로 나타내어 결과에 대한 원인 파악이 가능하다는 장점이 있다. 즉, 결정 트리의 규칙들이 어떤 기준으로 설정되었는지 살펴보고 분류 결과 확인이 같이 가능하기 때문에 분류 기준을 파악하기에 유용할것이다.]]></summary></entry><entry><title type="html">Logistic regression</title><link href="http://localhost:4000/machine%20learning/logistic_regression/" rel="alternate" type="text/html" title="Logistic regression" /><published>2023-02-04T00:00:00+09:00</published><updated>2023-02-04T00:00:00+09:00</updated><id>http://localhost:4000/machine%20learning/logistic_regression</id><content type="html" xml:base="http://localhost:4000/machine%20learning/logistic_regression/"><![CDATA[<p><br />
<strong>머신러닝은 크게 지도학습과 비지도학습 그리고 강화학습으로 구분된다.</strong> 그중에서 지도학습의 기법인 로지스틱 회귀(LogisticRegression)에 대해 알아보고자 한다. 지도학습은 크게 <strong>분류(Classification)와 회귀(Regression)로</strong> 구분짓는데 <strong>로지스틱 회귀는 이름에 회귀(Regression)을 포함하고 있지만 대부분 분류를 목적으로 활용된다.</strong> 어떤 이유로 이름에 회귀를 포함하고 있지만 분류를 목적으로 사용하고 있는지 알아보고 로지스틱 회귀분석이란 어떤 것인지 정리를 통해 구체적으로 알아보겠다.</p>

<p><br /></p>

<p>우선, 로지스틱 회귀를 알아보기 전에 회귀란 무엇인지 구체적인 개념을 잡고 가보자. <strong>회귀의 사전적 의미는 ‘한 바퀴 돌아서 본디의 자리나 상태로 돌아오는 것’이라고 한다.</strong> 분석에서 회귀는 평균으로 돌아가는 것을 뜻하며, 예를 들어 부모와 자녀의 키는 선형적인 관계가 있어서 부모의 키가 크면 자녀의 키도 크겠지만 부모의 키가 아무리 커도 <strong>결국 자녀의 키는 자녀 세대의 평균으로 돌아가고 있는 것을 회귀한다고 표현한다.</strong></p>

<p><img src="https://user-images.githubusercontent.com/82218035/116064424-f6d4b980-a6c0-11eb-9fa3-f4a83685b1e8.PNG" alt="lr3" /></p>

<p>로지스틱 회귀는 선형 회귀의 목표와 동일하게 종속 변수와 독립 변수의 관계를 하나의 모델로 정의하고 예측에 활용하는 것이다. 이렇듯 독립 변수의 선형 결합으로 종속 변수를 설명하고 있기에 선형 회귀 분석과 유사하지만 로지스틱 회귀는 종속 변수가 연속형이 아닌 범주형이라는 점에서 분류 기법으로 활용된다. <strong>즉, 독립 변수의 선형적 특징을 활용한다는 점이 이름에 회귀를 포함하고 있다고 보았고, 종속 변수가 범주형이기에 분류 기법으로 활용한다고 보여진다.</strong></p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/116064474-02c07b80-a6c1-11eb-9606-ac525f25e44f.PNG" alt="lr1" /></p>

<p>위의 그래프는 선형 회귀 분석을 통한 분류는 문제가 있음을 알수있다. 첫번째 그래프를 보면 22시간 이상 공부한 사람은 시험에 통과하였고 22시간 보다 적게 공부한 사람은 탈락하였다. 이 결과를 바탕으로 선형적인 모델을 만들면 경계선을 기준으로 22시간일때의 합격률은 0.6~0.7 정도가 나온다. 만약 여기서 새로운 데이터로 많은 시간을 공부해서 합격한 사람이 추가된다고 가정해보자. 결과값인  종속 변수가 최대값이 1이기 때문에 이때 만들어진 모델은 그 전 모델보다 기울기가 감소하게된다. 이때 문제가 생기는데 <strong>기울기가 감소하여 합격의 기준이 높아짐으로 그전에 합격이라고 예측했던 값들이 불합격으로 잘못 판단하게 되는 경우가 발생하게된다.</strong> 이러한 문제점을 해결하기 위해 발견한 함수가 시그모이드 함수로 로지스틱 회귀에 사용된다.</p>

<p><br /></p>

<p><img src="https://user-images.githubusercontent.com/82218035/116173868-4c07de00-a748-11eb-8ca1-44e4b11cc212.PNG" alt="lr4" /></p>

<p>시그모이드 함수는 무한한 범위의 x값이 들어오면 y값의 범위는 한상 [0, 1] 사이의 값으로 반환한다. 이러한 함수의 특징때문에 <strong>높은 값의 x가 들어와도 합격률의 변동이 없어 선형 회귀를 통한 분류의 문제를 해결가능하도록 만들어준다.</strong> 반대로 선형 회귀는 y값이 [0, 1]사이를 벗어나는 값이 나오기 때문에 분류로 활용시에는 예측의 정확도만 떨어뜨리게 된다. 그래서 로지스틱 회귀에서 시그모이드 함수를 사용하고 시그모이드 함수는 확률값을 반환해 0에 가까우면 0으로 1에 가까우면 1로 분류를 시행한다.</p>

<p><img src="https://user-images.githubusercontent.com/82218035/116173913-60e47180-a748-11eb-9f66-c5897a944856.PNG" alt="lr3" /></p>

<p><br /></p>

<h6 id="결론">결론</h6>
<ul>
  <li>선형회귀로 분류를 시행 시 발생하는 문제를 해결하기 위해 로지스틱 회귀를 사용한다.</li>
  <li>로지스틱 회귀는 시그모이드 함수를 사용한 분류 기법이다.</li>
  <li>시그모이드 함수는 확률값을 반환하고 확률에 따라 [0, 1]로 분류를 시행한다.</li>
</ul>]]></content><author><name>Moon Hyeongyu</name></author><category term="Machine learning" /><category term="Machine learning" /><category term="Model" /><category term="Backlog" /><summary type="html"><![CDATA[머신러닝은 크게 지도학습과 비지도학습 그리고 강화학습으로 구분된다. 그중에서 지도학습의 기법인 로지스틱 회귀(LogisticRegression)에 대해 알아보고자 한다. 지도학습은 크게 분류(Classification)와 회귀(Regression)로 구분짓는데 로지스틱 회귀는 이름에 회귀(Regression)을 포함하고 있지만 대부분 분류를 목적으로 활용된다. 어떤 이유로 이름에 회귀를 포함하고 있지만 분류를 목적으로 사용하고 있는지 알아보고 로지스틱 회귀분석이란 어떤 것인지 정리를 통해 구체적으로 알아보겠다.]]></summary></entry></feed>